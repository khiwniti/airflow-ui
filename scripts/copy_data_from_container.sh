#!/bin/bash
# Shell script to copy data from Airflow container to localhost
# This script copies all data files generated by DAGs to the local data_output directory

echo "🔄 Copying data from Airflow container to localhost..."

# Check if container is running
CONTAINER_NAME="synapes-analytics-airflow-scheduler-1"
CONTAINER_STATUS=$(docker ps --filter "name=$CONTAINER_NAME" --format "{{.Status}}")

if [ -z "$CONTAINER_STATUS" ]; then
    echo "❌ Container $CONTAINER_NAME is not running!"
    echo "Please start the Airflow services first with: docker-compose up -d"
    exit 1
fi

echo "✅ Container $CONTAINER_NAME is running"

# Create local data_output directory if it doesn't exist
if [ ! -d "data_output" ]; then
    mkdir -p data_output
    echo "📁 Created local data_output directory"
fi

# Copy data from container to localhost
echo "📥 Copying data files from container..."

if docker cp "${CONTAINER_NAME}:/opt/airflow/data_output/." "./data_output/"; then
    echo "✅ Data files copied successfully!"
    
    # List the files that were copied
    if [ "$(ls -A data_output 2>/dev/null)" ]; then
        echo ""
        echo "📊 Data files in localhost:"
        echo "=================================================="
        
        # List files with details
        ls -la data_output/ | grep -v "^total" | grep -v "^d" | while read -r line; do
            filename=$(echo "$line" | awk '{print $9}')
            size=$(echo "$line" | awk '{print $5}')
            date=$(echo "$line" | awk '{print $6, $7, $8}')
            if [ -n "$filename" ] && [ "$filename" != "." ] && [ "$filename" != ".." ]; then
                echo "📄 $filename ($(echo "scale=2; $size/1024" | bc -l 2>/dev/null || echo "$size") KB) - $date"
            fi
        done
        
        echo "=================================================="
        
        # Count files
        file_count=$(find data_output -type f | wc -l)
        echo "📈 Total files: $file_count"
        
        # Show summary by file type
        json_count=$(find data_output -name "*.json" -type f | wc -l)
        csv_count=$(find data_output -name "*.csv" -type f | wc -l)
        
        echo ""
        echo "📋 File Summary:"
        echo "   JSON files: $json_count"
        echo "   CSV files: $csv_count"
        
        # Show latest files
        echo ""
        echo "🕒 Latest 5 files:"
        find data_output -type f -printf '%T@ %p\n' | sort -nr | head -5 | while read -r timestamp filepath; do
            filename=$(basename "$filepath")
            echo "   • $filename"
        done
        
    else
        echo "ℹ️ No data files found in container"
        echo "Run a DAG first to generate data files"
    fi
    
else
    echo "❌ Failed to copy data files"
    exit 1
fi

echo ""
echo "🎉 Data copy completed successfully!"
echo "💾 Data files are now available in: $(pwd)/data_output"

# Optional: Show how to view the data
echo ""
echo "💡 Tips:"
echo "   • View JSON files: cat data_output/*.json | jq ."
echo "   • View CSV files: cat data_output/*.csv"
echo "   • Monitor new files: ls -lt data_output/"
